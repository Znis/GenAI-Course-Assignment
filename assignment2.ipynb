{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "84b23285",
      "metadata": {},
      "source": [
        "# <b>Getting started with Generative-AI </b>\n",
        "Submitted By: <b><i>Jenish Twayana</i></b>  \n",
        "\n",
        "Submitted Date: <b><i>2nd September, 2024</i></b>  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ded0e7c",
      "metadata": {},
      "source": [
        "## <b>Exercise 2: Few-shot Prompting and Evaluation</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17c931bc",
      "metadata": {},
      "source": [
        "### Set up the client and function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a3384995",
      "metadata": {},
      "outputs": [],
      "source": [
        "#importing necessary packages\n",
        "from openai import OpenAI\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import json\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4e81c1a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "#initialize openai client\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "model = \"gpt-3.5-turbo\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2c1e07b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "#output parser\n",
        "def parse_output(ai_message: str) -> str:\n",
        "    \"\"\"Parse the AI message.\"\"\"\n",
        "    return ai_message.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "effcb8a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "#function for calling api\n",
        "def get_completion(system_prompt, user_prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "    response = client.chat.completions.create(\n",
        "    model=model,\n",
        "    messages=messages,\n",
        "    temperature=0,\n",
        "    )\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54a34b66",
      "metadata": {},
      "source": [
        "### Task 1 (Text Classification):  \n",
        "\n",
        "Create a few-shot prompt that classifies given statements into one of three categories: factual,\n",
        "opinion, or ambiguous. Note that 'factual' does not necessarily mean 'true.' Display the output\n",
        "for each test case in JSON format. Ensure that the test cases are not included in the prompt to\n",
        "avoid data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2ce87d81",
      "metadata": {},
      "outputs": [],
      "source": [
        "#desired response for text classification in valid JSON format\n",
        "text_classification_response_example = '''\n",
        "<examples>\n",
        "<example1>\n",
        "Prompt: \"The Earth is the third planet from the Sun.\"\n",
        "Desired Response in valid JSON format: {\n",
        "\"text\": \"The Earth is the third planet from the Sun.\",\n",
        "\"label\": \"Factual\"\n",
        "}\n",
        "</example1>\n",
        "<example2>\n",
        "Prompt: \"Swimming is fun.\"\n",
        "Desired Response in valid JSON format: {\n",
        "\"text\": \"Swimming is fun.\",\n",
        "\"label\": \"Opinion\"\n",
        "}\n",
        "</example2>\n",
        "<example3>\n",
        "Prompt: \"He might not come early.\"\n",
        "Desired Response in valid JSON format: {\n",
        "\"text\": \"He might not come early.\",\n",
        "\"label\": \"Ambiguous\"\n",
        "}\n",
        "</example3>\n",
        "</examples>\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3b5eaf24",
      "metadata": {},
      "outputs": [],
      "source": [
        "#system prompt for text classification\n",
        "text_classification_task_system_prompt = f'''\n",
        "Classify the following text into one of the following categories: factual, opinion or ambiguous.\n",
        "Here are the few examples which are delimited by <examples></examples>:\n",
        "{text_classification_response_example}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "21548a18-06bc-4538-a477-167be6485011",
      "metadata": {
        "id": "21548a18-06bc-4538-a477-167be6485011"
      },
      "outputs": [],
      "source": [
        "# test dataset for text classification\n",
        "text_classification_data = [\n",
        "    (\"The capital of France is Paris.\", \"Factual\"),\n",
        "    (\"I believe chocolate is the best dessert.\", \"Opinion\"),\n",
        "    (\"It might rain tomorrow.\", \"Ambiguous\"),\n",
        "    (\"Mount Everest is the highest mountain in the world.\", \"Factual\"),\n",
        "    (\"In my opinion, summer is the best season.\", \"Opinion\"),\n",
        "    (\"She may arrive at the party tonight.\", \"Ambiguous\"),\n",
        "    (\"Pizza is delicious.\", \"Opinion\"),\n",
        "    (\"The Earth orbits around the Sun.\", \"Factual\"),\n",
        "    (\"Sunsets are beautiful.\", \"Opinion\"),\n",
        "    (\"Water boils at 100 degrees Celsius.\", \"Factual\"),\n",
        "    (\"Classical music is soothing.\", \"Opinion\"),\n",
        "    (\"The meeting could be postponed.\", \"Ambiguous\"),\n",
        "    (\"Football is the most popular sport globally.\", \"Opinion\"),\n",
        "    (\"She could decide to travel abroad.\", \"Ambiguous\"),\n",
        "    (\"Ice cream is better than cake.\", \"Opinion\")\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "04ec0e41",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "\"text\": \"The capital of France is Paris.\",\n",
            "\"label\": \"Factual\"\n",
            "}\n",
            "\n",
            "\n",
            "{\n",
            "\"text\": \"I believe chocolate is the best dessert.\",\n",
            "\"label\": \"Opinion\"\n",
            "}\n",
            "\n",
            "\n",
            "{\n",
            "\"text\": \"It might rain tomorrow.\",\n",
            "\"label\": \"Ambiguous\"\n",
            "}\n",
            "\n",
            "\n",
            "{\n",
            "\"text\": \"Mount Everest is the highest mountain in the world.\",\n",
            "\"label\": \"Factual\"\n",
            "}\n",
            "\n",
            "\n",
            "{\n",
            "\"text\": \"In my opinion, summer is the best season.\",\n",
            "\"label\": \"Opinion\"\n",
            "}\n",
            "\n",
            "\n",
            "{\n",
            "\"text\": \"She may arrive at the party tonight.\",\n",
            "\"label\": \"Ambiguous\"\n",
            "}\n",
            "\n",
            "\n",
            "{\n",
            "\"text\": \"Pizza is delicious.\",\n",
            "\"label\": \"Opinion\"\n",
            "}\n",
            "\n",
            "\n",
            "{\n",
            "\"text\": \"The Earth orbits around the Sun.\",\n",
            "\"label\": \"Factual\"\n",
            "}\n",
            "\n",
            "\n",
            "{\n",
            "\"text\": \"Sunsets are beautiful.\",\n",
            "\"label\": \"Opinion\"\n",
            "}\n",
            "\n",
            "\n",
            "{\n",
            "\"text\": \"Water boils at 100 degrees Celsius.\",\n",
            "\"label\": \"Factual\"\n",
            "}\n",
            "\n",
            "\n",
            "{\n",
            "\"text\": \"Classical music is soothing.\",\n",
            "\"label\": \"Opinion\"\n",
            "}\n",
            "\n",
            "\n",
            "{\n",
            "\"text\": \"The meeting could be postponed.\",\n",
            "\"label\": \"Ambiguous\"\n",
            "}\n",
            "\n",
            "\n",
            "{\n",
            "\"text\": \"Football is the most popular sport globally.\",\n",
            "\"label\": \"Factual\"\n",
            "}\n",
            "\n",
            "\n",
            "{\n",
            "\"text\": \"She could decide to travel abroad.\",\n",
            "\"label\": \"Ambiguous\"\n",
            "}\n",
            "\n",
            "\n",
            "{\n",
            "\"text\": \"Ice cream is better than cake.\",\n",
            "\"label\": \"Opinion\"\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#generate responses and store in list\n",
        "text_classification_responses_json = []\n",
        "for text in text_classification_data:\n",
        "    response = get_completion(text_classification_task_system_prompt, text[0])\n",
        "    response = parse_output(response)\n",
        "    print(response)\n",
        "    print(\"\\n\")\n",
        "    text_classification_responses_json.append(json.loads(response)) #parse the response to json and store in list"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e046b41d",
      "metadata": {},
      "source": [
        "### Task 2 (Text Classification Evaluation): \n",
        "\n",
        "Evaluate the outputs with the ground truth(Expected Answer) provided. Calculate the accuracy\n",
        "of your system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "69c3428e",
      "metadata": {},
      "outputs": [],
      "source": [
        "#function to calculate text classification evaluation\n",
        "def get_text_classification_evaluation(text_classification_responses_json, text_classification_data):\n",
        "    correct = incorrect = 0\n",
        "    for response in text_classification_responses_json:\n",
        "        if response['label'] == text_classification_data[text_classification_responses_json.index(response)][1]:\n",
        "            correct += 1\n",
        "        else:\n",
        "            incorrect += 1\n",
        "    evaluation_output = {\n",
        "        \"Total_num_text\": len(text_classification_responses_json),\n",
        "        \"Total_correct_predictions\": correct,\n",
        "        \"Total_incorrect_predictions\": incorrect,\n",
        "        \"Overall_accuracy\": correct/len(text_classification_responses_json)\n",
        "    }\n",
        "    return evaluation_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bf2b78d7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Total_num_text': 15, 'Total_correct_predictions': 14, 'Total_incorrect_predictions': 1, 'Overall_accuracy': 0.9333333333333333}\n"
          ]
        }
      ],
      "source": [
        "print(get_text_classification_evaluation(text_classification_responses_json, text_classification_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df5fd413",
      "metadata": {},
      "source": [
        "### Task 3 (Logical Reasoning and Evaluation):  \n",
        "\n",
        "Create a Few-shot Prompt that can Identify the conclusion from the given premises. Display the\n",
        "output in JSON format for all the test cases. Do not feed the test cases into the prompt (Data\n",
        "Leakage). Evaluate the answers using BLEU Score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e5f547cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "#desired response for logical reasoningin valid JSON format\n",
        "logical_reasoning_response_example = '''\n",
        "<examples>\n",
        "<example1>\n",
        "Prompt: \"All birds have wings. A sparrow is a bird.\"\n",
        "Desired Response in valid JSON format: {\n",
        "\"premises\": \"All birds have wings. A sparrow is a bird.\",\n",
        "\"conclusion\": \"A sparrow has wings\"\n",
        "}\n",
        "</example1>\n",
        "<example2>\n",
        "Prompt: \"If the engine is turned on, the car will move. The engine is turned on.\"\n",
        "Desired Response in valid JSON format: {\n",
        "\"premises\": \"If the engine is turned on, the car will move. The engine is turned on.\",\n",
        "\"conclusion\": \"The car is moving.\"\n",
        "}\n",
        "</example2>\n",
        "<example3>\n",
        "Prompt: \"If it is raining outside, he brings an umbrella. It is raining outside.\"\n",
        "Desired Response in valid JSON format: {\n",
        "\"premises\": \"If it is raining outside, he brings an umbrella. It is raining outside.\",\n",
        "\"conclusion\": \"he brings an umbrella.\"\n",
        "}\n",
        "</example3>\n",
        "</examples>\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "74bc9b0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "#system prompt for logical reasoning\n",
        "logical_reasoning_task_system_prompt = f'''\n",
        "Identify the conclusion from the given premises.\n",
        "Here are the few examples delimited by <examples></examples>:\n",
        "{logical_reasoning_response_example}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3180341a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# test dataset for logical reasoning\n",
        "logical_reasoning_data =[\n",
        "    (\"If it rains, the streets will be wet. It is raining.\", \"The streets are wet.\"),\n",
        "    (\"All squares are rectangles. This shape is a square.\", \"This shape is a rectangle.\"),\n",
        "    (\"If it is sunny, she will go for a walk. It is sunny.\", \"She will go for a walk.\"),\n",
        "    (\"All students must pass the final exam. John is a student.\", \"John must pass the final exam.\"),\n",
        "    (\"If the oven is turned on, the food will cook. The oven is turned on.\", \"The food is cooking.\"),\n",
        "    (\"All prime numbers are odd. Seven is a prime number.\", \"Seven is odd.\"),\n",
        "    (\"If he studies hard, he will pass the test. He studied hard.\", \"He will pass the test.\"),\n",
        "    (\"All mammals have hair. Whales are mammals.\", \"Whales have hair.\"),\n",
        "    (\"If it is cold outside, she wears a jacket. It is cold outside.\", \"She wears a jacket.\"),\n",
        "    (\"All metals conduct electricity. Copper is a metal.\", \"Copper conducts electricity.\")\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e6eef3c8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "\"premises\": \"If it rains, the streets will be wet. It is raining.\",\n",
            "\"conclusion\": \"The streets are wet.\"\n",
            "}\n",
            "\n",
            "\n",
            "{\n",
            "\"premises\": \"All squares are rectangles. This shape is a square.\",\n",
            "\"conclusion\": \"This shape is a rectangle.\"\n",
            "}\n",
            "\n",
            "\n",
            "{\n",
            "\"premises\": \"If it is sunny, she will go for a walk. It is sunny.\",\n",
            "\"conclusion\": \"She will go for a walk.\"\n",
            "}\n",
            "\n",
            "\n",
            "{\n",
            "\"premises\": \"All students must pass the final exam. John is a student.\",\n",
            "\"conclusion\": \"John must pass the final exam.\"\n",
            "}\n",
            "\n",
            "\n",
            "{\n",
            "\"premises\": \"If the oven is turned on, the food will cook. The oven is turned on.\",\n",
            "\"conclusion\": \"The food is cooking.\"\n",
            "}\n",
            "\n",
            "\n",
            "{\n",
            "\"premises\": \"All prime numbers are odd. Seven is a prime number.\",\n",
            "\"conclusion\": \"Seven is odd.\"\n",
            "}\n",
            "\n",
            "\n",
            "{\n",
            "\"premises\": \"If he studies hard, he will pass the test. He studied hard.\",\n",
            "\"conclusion\": \"He will pass the test.\"\n",
            "}\n",
            "\n",
            "\n",
            "{\n",
            "\"premises\": \"All mammals have hair. Whales are mammals.\",\n",
            "\"conclusion\": \"Whales have hair.\"\n",
            "}\n",
            "\n",
            "\n",
            "{\n",
            "\"premises\": \"If it is cold outside, she wears a jacket. It is cold outside.\",\n",
            "\"conclusion\": \"She wears a jacket.\"\n",
            "}\n",
            "\n",
            "\n",
            "{\n",
            "\"premises\": \"All metals conduct electricity. Copper is a metal.\",\n",
            "\"conclusion\": \"Copper conducts electricity.\"\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#generate responses and store in list\n",
        "logical_reasoning_responses_json = []\n",
        "for text in logical_reasoning_data:\n",
        "    response = get_completion(logical_reasoning_task_system_prompt, text[0])\n",
        "    response = parse_output(response)\n",
        "    print(response)\n",
        "    print(\"\\n\")\n",
        "    logical_reasoning_responses_json.append(json.loads(response)) #parse the response to json and store in list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "63689da0-fd22-4143-8043-99d02c0d41aa",
      "metadata": {
        "id": "63689da0-fd22-4143-8043-99d02c0d41aa"
      },
      "outputs": [],
      "source": [
        "# function to evaluate bleu score for each test example\n",
        "def evaluate_bleu_score(hypothesis, reference):\n",
        "    if hypothesis.strip()[-1] == \".\":\n",
        "        hypothesis = hypothesis.strip()[:-1] #remove the whitespaces before and after the sentence and last period if present\n",
        "    if reference.strip()[-1] == \".\":\n",
        "        reference = reference.strip()[:-1] #remove the whitespaces before and after the sentence and last period if present\n",
        "    BLEUscore = sentence_bleu([reference], hypothesis)\n",
        "    return BLEUscore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "886199ee",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Case Conclusion: The streets are wet.\n",
            "Predicted Conclusion: The streets are wet.\n",
            "BLEUscore: 1.0\n",
            "\n",
            "\n",
            "Test Case Conclusion: This shape is a rectangle.\n",
            "Predicted Conclusion: This shape is a rectangle.\n",
            "BLEUscore: 1.0\n",
            "\n",
            "\n",
            "Test Case Conclusion: She will go for a walk.\n",
            "Predicted Conclusion: She will go for a walk.\n",
            "BLEUscore: 1.0\n",
            "\n",
            "\n",
            "Test Case Conclusion: John must pass the final exam.\n",
            "Predicted Conclusion: John must pass the final exam.\n",
            "BLEUscore: 1.0\n",
            "\n",
            "\n",
            "Test Case Conclusion: The food is cooking.\n",
            "Predicted Conclusion: The food is cooking.\n",
            "BLEUscore: 1.0\n",
            "\n",
            "\n",
            "Test Case Conclusion: Seven is odd.\n",
            "Predicted Conclusion: Seven is odd.\n",
            "BLEUscore: 1.0\n",
            "\n",
            "\n",
            "Test Case Conclusion: He will pass the test.\n",
            "Predicted Conclusion: He will pass the test.\n",
            "BLEUscore: 1.0\n",
            "\n",
            "\n",
            "Test Case Conclusion: Whales have hair.\n",
            "Predicted Conclusion: Whales have hair.\n",
            "BLEUscore: 1.0\n",
            "\n",
            "\n",
            "Test Case Conclusion: She wears a jacket.\n",
            "Predicted Conclusion: She wears a jacket.\n",
            "BLEUscore: 1.0\n",
            "\n",
            "\n",
            "Test Case Conclusion: Copper conducts electricity.\n",
            "Predicted Conclusion: Copper conducts electricity.\n",
            "BLEUscore: 1.0\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for response in logical_reasoning_responses_json:\n",
        "    print(f\"Test Case Conclusion: {logical_reasoning_data[logical_reasoning_responses_json.index(response)][1]}\")\n",
        "    print(f\"Predicted Conclusion: {response['conclusion']}\")\n",
        "    print(f\"BLEUscore: {evaluate_bleu_score(logical_reasoning_data[logical_reasoning_responses_json.index(response)][1], response['conclusion'])}\")\n",
        "    print(\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "genai-assignment-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
